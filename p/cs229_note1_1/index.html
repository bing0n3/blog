<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content=bing0ne><meta name=description content="线性回归是Ng Andrew在CS229中讲的第一个模型. 这里我们来解析一下他."><meta name=keywords content=blog,bing0ne,gopher,gsoc><link rel=canonical href=https://blog.bing0ne.com/p/cs229_note1_1/><title>CS229: 线性回归,梯度下降,正规方程 &middot; bing0ne Base</title><link rel="shortcut icon" href=https://blog.bing0ne.com/images/favicon.ico><link rel=stylesheet href=https://blog.bing0ne.com/css/animate.min.css><link rel=stylesheet href=https://blog.bing0ne.com/css/remixicon.css><link rel=stylesheet href=https://blog.bing0ne.com/css/zozo.css><link rel=stylesheet href=https://blog.bing0ne.com/css/highlight.css></head><body><div class="main animated"><div class="nav_container animated fadeInDown"><div class=site_nav id=site_nav><ul><li><a href=/>Home</a></li><li><a href=/posts/>Archive</a></li><li><a href=/tags/>Tags</a></li></ul></div><div class=menu_icon><a id=menu_icon><i class=remixicon-links-line></i></a></div></div><div class="header animated fadeInDown"><div class=site_title_container><div class=site_title><h1><a href=https://blog.bing0ne.com><span>bing0ne Base</span>
<img src=https://blog.bing0ne.com/images/logo.svg></a></h1></div><div class=description><p class=sub_title>Neverland</p><div class=my_socials><a href=https://github.com/bing0n3 title=github target=_blank><i class=remixicon-github-fill></i></a><a href=https://instagram.com/bing0n3 title=instagram target=_blank><i class=remixicon-instagram-fill></i></a><a href=https://twitter.com/bing0n3 title=twitter target=_blank><i class=remixicon-twitter-fill></i></a><a href=https://blog.bing0ne.com/index.xml type=application/rss+xml title=rss target=_blank><i class=remixicon-rss-fill></i></a></div></div></div></div><div class=content><div class=post_page><div class="post animated fadeInDown"><div class="post_title post_detail_title"><h2><a href=/p/cs229_note1_1/>CS229: 线性回归,梯度下降,正规方程</a></h2><span class=date>2017.07.13</span></div><div class="post_content markdown"><p>线性回归是Ng Andrew在CS229中讲的第一个模型. 这里我们来解析一下他.</p><p>线性回归这个概念我看很多人说高中就学过了,虽然我并没有什么印象. 为了帮助大家的理解这里解释一下线性回归这个概念.<strong>线性回归</strong>是利用称为线性回归方程的<strong>最小平方函數</strong>对一个或多个自变量和因变量之间关系进行建模的一种回归分析. 在线性回归函数中, 数据使用线性预测函数来建模, 并且未知的模型参数也是通过数据来估计.</p><p>线性预测函数:</p><p>$$h_\theta = \theta_0+\theta_1x_1+\theta_2x_2+... = \theta^TX$$</p><p>其中$\theta$在这里是参数的意思,表示这些feature在线性预测函数中的份量.我们也可以用向量的方法表示这一个函数</p><p>为了使得我们的预测结果准确,显然我们需要一个机制去评估我们的预测结果, 所以这里提出了<strong>代价函数</strong>来评估$\theta$是否合适.</p><p>$$
J(\theta) = \frac{1}{2} \sum<em>{i=1}^m(h</em>\theta(x^{(i)})-y^{(i)})^2
$$</p><p>这个错误估计函数是去对$x^{(i)}$的估计值与真实值$y^{(i)}$差的平方和作为错误估计函数，前面乘上的1/2是为了方便求导的.</p><h2 id=梯度下降>梯度下降</h2><p>根据我们前面的代价函数,我们显然需要找到一个$\theta$来使得我们的代价函数的值最小.</p><h3 id=lms-最小均方算法>LMS(最小均方算法)</h3><p>根据LMS,也就是最小均算法,我们提出公式$\theta_j:=\theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta)$.现在我们就要解出等式右边的偏导项.</p><p>首先假设我们只有一个训练样本，这样我们就可以忽略求和符号。</p><p><img src=https://ws2.sinaimg.cn/bmiddle/006tNc79gy1fm1ngk9p0cj30ny0bsdh6.jpg alt></p><p>因此我们得到了我们的更新$\theta$的规则:
$$\theta_j := \theta<em>j - \alpha(h</em>\theta(x^{(i)})-y^{(i)})x_j^{i}$$</p><p>我们现在的得到的更新规则只是针对一个训练样本的结果. 我们对这个进行一些修改就能够得到针对更多数据的两种梯度下降方法.</p><h3 id=批量梯度下降-batch-gradient-descent>批量梯度下降(Batch Gradient Descent)</h3><p>如同这个名字一般,我们的每一次下降都会把所有的数据都考虑进去,一同处理.</p><p>重复到收敛</p><p><img src=https://ws4.sinaimg.cn/bmiddle/006tNc79gy1fm1ndbul67j30oo03yweu.jpg alt></p><p>我们可以看到函数,其实比于本来的更新函数,只是多了一个SUM,其实也就是$J(\theta)$的展开不同,造成的差异.</p><p>通常来说梯度下降可能会得到一个局部最优解，但是对于线性回归来说，BGD总是会收敛到全局最优解(因为线性回归的代价函数是一个凸函数),当然这也有前提条件那便是$\alpha$的值要恰当,不能过大.</p><h3 id=代码实现>代码实现</h3><pre><code class=language-python>import matplotlib.pyplot as plt
from numpy import *

# 读取数据集
def loadDataSet(filename):
    file_stream = open(filename)
    num_feature = len(file_stream.readline().split('\t')) - 1
    data_matrix = []
    label_matrix = []
    for line in file_stream.readlines():
        line_array = []
        # strip remove leading and tail char
        current_line = line.strip().split(&quot;\t&quot;)
        for i in range(num_feature):
            line_array.append(float(current_line[i]))
        data_matrix.append(line_array)
        label_matrix.append(float(current_line[-1]))
    return data_matrix, label_matrix

# 梯度下降
def gradient_descent(xVec,yVec):
    epsilon = 0.0005;
    alpha= 0.0001;
    X = mat(xVec)
    Y = mat(yVec).T
    theta = mat(ones(2)).T
    cost_funtion = []
    while True:
        # print(theta[0])
        J = 1/(2* len(X[0])) * (X.dot(theta) - Y ).T * (X.dot(theta) - Y )
        print(J)
        cost_funtion.append(float(J))
        theta_new = theta - alpha * (X.T * (X*theta - Y))
        if (theta_new[0] - theta[0]) &lt; epsilon and (theta_new[1] - theta[1]) &lt; epsilon:
            return theta_new, cost_funtion
        theta = theta_new

# 画图
def plotPoint(linear_regress_funciton):
    xVec, yVec = loadDataSet('ex0.txt')
    X = mat(xVec)
    Y = mat(yVec)
    fig = plt.figure()
    ax = fig.add_subplot(1, 1, 1)
    ax.scatter(X[:, 1].flatten().A[0], Y.T[:, 0].flatten().A[0])
    xCopy = X.copy()
    xCopy.sort(0) # sort(0) 中的0指定排序参数
    theta = linear_regress_funciton(xVec, yVec)[0]
    print(theta)
    yHat = xCopy * theta
    ax.plot(xCopy[:, 1], yHat)
    plt.show()

# cost function 的下降曲线
def plot_cost_funciton():
    xVec, yVec = loadDataSet('ex0.txt')
    X = mat(xVec)
    Y = mat(yVec)
    fig = plt.figure()
    ax = fig.add_subplot(1, 1, 1)
    j_cost = gradient_descent(xVec,yVec)[1]
    ax.scatter(range(0,len(j_cost)), j_cost)
    ax.plot(range(0,len(j_cost)),j_cost)
    plt.show()

plotPoint(gradient_descent)
plot_cost_funciton()
</code></pre><h3 id=随机梯度下降-stochastic-gradient-descent>随机梯度下降(Stochastic Gradient Descent)</h3><p>Loop {
for i = 1 to n {
<img src=https://ws2.sinaimg.cn/bmiddle/006tNc79gy1fm1n4wcyhbj30p604yt91.jpg alt>
&nbsp;&nbsp;&nbsp;&nbsp;}
}</p><h3 id=两种梯度下降算法的比较>两种梯度下降算法的比较</h3><p>batch gradient descent的每一步都要遍历整个训练集,所以当我们的训练样本的量很大的时候,算法的开销很大;而stochastic gradient descent则每一步只选取了一个样本,因此后者的开销比前者要小很多,显然速度也要更快. 当然, 后者在有些情况下不会收敛,但是对于大多数的时间中,后者得到的结果都是真实最小值的一个足够好的近似结果</p><h2 id=正规方程>正规方程</h2><p>梯度下降是一种的到最小值的方法,但是我们有另一种方法,不需要通过迭代,一次性便可以得到最小值,那便是正规方程. 我们通过对J进行求导,然后令其为0.</p><p>$$J(\theta) = \sum<em>{i=0}^m(h</em>\theta(x^{(i)})-y^{(i)})^2 = \frac{1}{2}(\theta^TX-y)^T(\theta^TX-y) $$</p><p>$$\nabla_{\theta}J(\theta)= 0$$</p><p>求解:
<img src=https://ws2.sinaimg.cn/bmiddle/006tNc79gy1fm1n3vpz95j31940s0n3b.jpg alt></p><h3 id=正规方程代码实现>正规方程代码实现</h3><p>参考了机器学习实战的代码实现</p><pre><code class=language-python>import matplotlib.pyplot as plt
from numpy import *

# 读取数据集
def loadDataSet(filename):
    file_stream = open(filename)
    num_feature = len(file_stream.readline().split('\t')) - 1
    data_matrix = []
    label_matrix = []
    for line in file_stream.readlines():
        line_array = []
        # strip remove leading and tail char
        current_line = line.strip().split(&quot;\t&quot;)
        for i in range(num_feature):
            line_array.append(float(current_line[i]))
        data_matrix.append(line_array)
        label_matrix.append(float(current_line[-1]))
    return data_matrix, label_matrix

# 根据正规方程求theta
def normalE_theta(xVec, yVec):
    X = mat(xVec)
    Y = mat(yVec).T
    XTX = X.T * X
    if linalg.det(XTX) == 0:  # 计算行列式,如果为奇异矩阵则不能求逆矩阵
        print('This is a singular matrix, it canot be inverse')
        return
    theta = XTX.I * X.T * Y
    return theta

# 画图
def plotPoint():
    xVec, yVec = loadDataSet('ex0.txt')
    X = mat(xVec)
    Y = mat(yVec)
    fig = plt.figure()
    ax = fig.add_subplot(1, 1, 1)
    ax.scatter(X[:, 1].flatten().A[0], Y.T[:, 0].flatten().A[0])
    xCopy = X.copy()
    xCopy.sort(0) # sort(0) 中的指定排序方式
    theta = normalE_theta(xVec, yVec)
    print(theta)
    yHat = xCopy * theta
    ax.plot(xCopy[:, 1], yHat)
    plt.show()

plotPoint()
</code></pre></div><div class=post_footer><div class=meta><div class=info><span class="field tags"><i class=remixicon-stack-line></i><a href=https://blog.bing0ne.com/tags/ml/>ML</a></span></div></div></div></div></div></div><a id=back_to_top href=# class=back_to_top><span>△</span></a></div><footer class=footer><div class=powered_by><a href=https://zeuk.me>Designed by Zeuk,</a>
<a href=http://www.gohugo.io/>Proudly published with Hugo</a></div><div class=footer_slogan><span>Share Memory by Communication</span></div></footer><script src=https://blog.bing0ne.com/js/jquery-3.3.1.min.js></script><script src=https://blog.bing0ne.com/js/zozo.js></script><script src=https://blog.bing0ne.com/js/highlight.pack.js></script><link href=https://blog.bing0ne.com/css/fancybox.min.css rel=stylesheet><script src=https://blog.bing0ne.com/js/fancybox.min.js></script><script>hljs.initHighlightingOnLoad()</script><script type=text/javascript async src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\[\[','\]\]']],processEscapes:true,processEnvironments:true,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}});MathJax.Hub.Queue(function(){var all=MathJax.Hub.getAllJax(),i;for(i=0;i<all.length;i+=1){all[i].SourceElement().parentNode.className+=' has-jax';}});</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-71832126-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"bing0ne"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></body></html>