<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    
    <title>CS229: 线性回归,梯度下降,正规方程 - bing0ne Base</title>
    
    <meta name="description" content="线性回归是Ng Andrew在CS229中讲的第一个模型. 这里我们来解析一下他.">
    <meta name="author" content="bing0ne">
    
    <link href="https://blog.bing0ne.com/an-old-hope.min.css" rel="stylesheet">
    <link href="https://blog.bing0ne.com/style.css" rel="stylesheet">
    
    <link rel="apple-touch-icon" href="https://blog.bing0ne.com/apple-touch-icon.png">
    <link rel="icon" href="https://blog.bing0ne.com/favicon.ico">
    
    <meta name="generator" content="Hugo 0.59.0-DEV" />
    
    <link rel="alternate" type="application/atom+xml" href="https://blog.bing0ne.com/index.xml" title="bing0ne Base">
    
    
    <meta property="og:title" content="CS229: 线性回归,梯度下降,正规方程" />
<meta property="og:description" content="线性回归是Ng Andrew在CS229中讲的第一个模型. 这里我们来解析一下他." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.bing0ne.com/p/cs229_note1_1/" />
<meta property="article:published_time" content="2017-07-13T10:56:21+08:00" />
<meta property="article:modified_time" content="2017-07-13T10:56:21+08:00" />

  </head>
  <body class="single">
    <header class="header">
      <nav class="nav">
        
        <p class="logo"><a href="https://blog.bing0ne.com">bing0ne Base</a></p>
        
        
        <ul class="menu">
          
          <li>
            <a href="https://blog.bing0ne.com/">Home</a>
          </li>
          
          <li>
            <a href="https://blog.bing0ne.com/categories/dev/">Dev</a>
          </li>
          
          <li>
            <a href="https://blog.bing0ne.com/categories/eassy/">Eassy</a>
          </li>
          
          <li>
            <a href="https://blog.bing0ne.com/categories/gsoc/">GSoC</a>
          </li>
          
          <li>
            <a href="https://blog.bing0ne.com/tags/">Tags</a>
          </li>
          
        </ul>
        
      </nav>
    </header>
    <main class="main">


<article class="post-single">
  <header class="post-header">
    <h1 class="post-title">CS229: 线性回归,梯度下降,正规方程</h1>
    <div class="post-meta">bing0ne · July 13, 2017</div>
  </header>
  <div class="post-content"><p>线性回归是Ng Andrew在CS229中讲的第一个模型. 这里我们来解析一下他.</p>

<p>线性回归这个概念我看很多人说高中就学过了,虽然我并没有什么印象. 为了帮助大家的理解这里解释一下线性回归这个概念.<strong>线性回归</strong>是利用称为线性回归方程的<strong>最小平方函數</strong>对一个或多个自变量和因变量之间关系进行建模的一种回归分析. 在线性回归函数中, 数据使用线性预测函数来建模, 并且未知的模型参数也是通过数据来估计.</p>

<p>线性预测函数:</p>

<p>$$h_\theta = \theta_0+\theta_1x_1+\theta_2x_2+&hellip; = \theta^TX$$</p>

<p>其中$\theta$在这里是参数的意思,表示这些feature在线性预测函数中的份量.我们也可以用向量的方法表示这一个函数</p>

<p>为了使得我们的预测结果准确,显然我们需要一个机制去评估我们的预测结果, 所以这里提出了<strong>代价函数</strong>来评估$\theta$是否合适.</p>

<p>$$
J(\theta) = \frac{1}{2} \sum<em>{i=1}^m(h</em>\theta(x^{(i)})-y^{(i)})^2
$$</p>

<p>这个错误估计函数是去对$x^{(i)}$的估计值与真实值$y^{(i)}$差的平方和作为错误估计函数，前面乘上的1/2是为了方便求导的.</p>

<h2 id="梯度下降">梯度下降</h2>

<p>根据我们前面的代价函数,我们显然需要找到一个$\theta$来使得我们的代价函数的值最小.</p>

<h3 id="lms-最小均方算法">LMS(最小均方算法)</h3>

<p>根据LMS,也就是最小均算法,我们提出公式$\theta_j:=\theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta)$.现在我们就要解出等式右边的偏导项.</p>

<p>首先假设我们只有一个训练样本，这样我们就可以忽略求和符号。</p>

<p><img src="https://ws2.sinaimg.cn/bmiddle/006tNc79gy1fm1ngk9p0cj30ny0bsdh6.jpg" alt="" /></p>

<p>因此我们得到了我们的更新$\theta$的规则:
$$\theta_j := \theta<em>j - \alpha(h</em>\theta(x^{(i)})-y^{(i)})x_j^{i}$$</p>

<p>我们现在的得到的更新规则只是针对一个训练样本的结果. 我们对这个进行一些修改就能够得到针对更多数据的两种梯度下降方法.</p>

<h3 id="批量梯度下降-batch-gradient-descent">批量梯度下降(Batch Gradient Descent)</h3>

<p>如同这个名字一般,我们的每一次下降都会把所有的数据都考虑进去,一同处理.</p>

<p>重复到收敛</p>

<p><img src="https://ws4.sinaimg.cn/bmiddle/006tNc79gy1fm1ndbul67j30oo03yweu.jpg" alt="" /></p>

<p>我们可以看到函数,其实比于本来的更新函数,只是多了一个SUM,其实也就是$J(\theta)$的展开不同,造成的差异.</p>

<p>通常来说梯度下降可能会得到一个局部最优解，但是对于线性回归来说，BGD总是会收敛到全局最优解(因为线性回归的代价函数是一个凸函数),当然这也有前提条件那便是$\alpha$的值要恰当,不能过大.</p>

<h3 id="代码实现">代码实现</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">from</span> numpy <span style="color:#f92672">import</span> <span style="color:#f92672">*</span>

<span style="color:#75715e"># 读取数据集</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loadDataSet</span>(filename):
    file_stream <span style="color:#f92672">=</span> open(filename)
    num_feature <span style="color:#f92672">=</span> len(file_stream<span style="color:#f92672">.</span>readline()<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">&#39;</span>)) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>
    data_matrix <span style="color:#f92672">=</span> []
    label_matrix <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> line <span style="color:#f92672">in</span> file_stream<span style="color:#f92672">.</span>readlines():
        line_array <span style="color:#f92672">=</span> []
        <span style="color:#75715e"># strip remove leading and tail char</span>
        current_line <span style="color:#f92672">=</span> line<span style="color:#f92672">.</span>strip()<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">&#34;</span>)
        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num_feature):
            line_array<span style="color:#f92672">.</span>append(float(current_line[i]))
        data_matrix<span style="color:#f92672">.</span>append(line_array)
        label_matrix<span style="color:#f92672">.</span>append(float(current_line[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]))
    <span style="color:#66d9ef">return</span> data_matrix, label_matrix

<span style="color:#75715e"># 梯度下降</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gradient_descent</span>(xVec,yVec):
    epsilon <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0005</span>;
    alpha<span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0001</span>;
    X <span style="color:#f92672">=</span> mat(xVec)
    Y <span style="color:#f92672">=</span> mat(yVec)<span style="color:#f92672">.</span>T
    theta <span style="color:#f92672">=</span> mat(ones(<span style="color:#ae81ff">2</span>))<span style="color:#f92672">.</span>T
    cost_funtion <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">while</span> True:
        <span style="color:#75715e"># print(theta[0])</span>
        J <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>(<span style="color:#ae81ff">2</span><span style="color:#f92672">*</span> len(X[<span style="color:#ae81ff">0</span>])) <span style="color:#f92672">*</span> (X<span style="color:#f92672">.</span>dot(theta) <span style="color:#f92672">-</span> Y )<span style="color:#f92672">.</span>T <span style="color:#f92672">*</span> (X<span style="color:#f92672">.</span>dot(theta) <span style="color:#f92672">-</span> Y )
        <span style="color:#66d9ef">print</span>(J)
        cost_funtion<span style="color:#f92672">.</span>append(float(J))
        theta_new <span style="color:#f92672">=</span> theta <span style="color:#f92672">-</span> alpha <span style="color:#f92672">*</span> (X<span style="color:#f92672">.</span>T <span style="color:#f92672">*</span> (X<span style="color:#f92672">*</span>theta <span style="color:#f92672">-</span> Y))
        <span style="color:#66d9ef">if</span> (theta_new[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">-</span> theta[<span style="color:#ae81ff">0</span>]) <span style="color:#f92672">&lt;</span> epsilon <span style="color:#f92672">and</span> (theta_new[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">-</span> theta[<span style="color:#ae81ff">1</span>]) <span style="color:#f92672">&lt;</span> epsilon:
            <span style="color:#66d9ef">return</span> theta_new, cost_funtion
        theta <span style="color:#f92672">=</span> theta_new

<span style="color:#75715e"># 画图</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plotPoint</span>(linear_regress_funciton):
    xVec, yVec <span style="color:#f92672">=</span> loadDataSet(<span style="color:#e6db74">&#39;ex0.txt&#39;</span>)
    X <span style="color:#f92672">=</span> mat(xVec)
    Y <span style="color:#f92672">=</span> mat(yVec)
    fig <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>figure()
    ax <span style="color:#f92672">=</span> fig<span style="color:#f92672">.</span>add_subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
    ax<span style="color:#f92672">.</span>scatter(X[:, <span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>flatten()<span style="color:#f92672">.</span>A[<span style="color:#ae81ff">0</span>], Y<span style="color:#f92672">.</span>T[:, <span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>flatten()<span style="color:#f92672">.</span>A[<span style="color:#ae81ff">0</span>])
    xCopy <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>copy()
    xCopy<span style="color:#f92672">.</span>sort(<span style="color:#ae81ff">0</span>) <span style="color:#75715e"># sort(0) 中的0指定排序参数</span>
    theta <span style="color:#f92672">=</span> linear_regress_funciton(xVec, yVec)[<span style="color:#ae81ff">0</span>]
    <span style="color:#66d9ef">print</span>(theta)
    yHat <span style="color:#f92672">=</span> xCopy <span style="color:#f92672">*</span> theta
    ax<span style="color:#f92672">.</span>plot(xCopy[:, <span style="color:#ae81ff">1</span>], yHat)
    plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># cost function 的下降曲线</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plot_cost_funciton</span>():
    xVec, yVec <span style="color:#f92672">=</span> loadDataSet(<span style="color:#e6db74">&#39;ex0.txt&#39;</span>)
    X <span style="color:#f92672">=</span> mat(xVec)
    Y <span style="color:#f92672">=</span> mat(yVec)
    fig <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>figure()
    ax <span style="color:#f92672">=</span> fig<span style="color:#f92672">.</span>add_subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
    j_cost <span style="color:#f92672">=</span> gradient_descent(xVec,yVec)[<span style="color:#ae81ff">1</span>]
    ax<span style="color:#f92672">.</span>scatter(range(<span style="color:#ae81ff">0</span>,len(j_cost)), j_cost)
    ax<span style="color:#f92672">.</span>plot(range(<span style="color:#ae81ff">0</span>,len(j_cost)),j_cost)
    plt<span style="color:#f92672">.</span>show()

plotPoint(gradient_descent)
plot_cost_funciton()</code></pre></div>
<h3 id="随机梯度下降-stochastic-gradient-descent">随机梯度下降(Stochastic Gradient Descent)</h3>

<p>Loop {
    for i = 1 to n {
<img src="https://ws2.sinaimg.cn/bmiddle/006tNc79gy1fm1n4wcyhbj30p604yt91.jpg" alt="" />
&nbsp;&nbsp;&nbsp;&nbsp;}
}</p>

<h3 id="两种梯度下降算法的比较">两种梯度下降算法的比较</h3>

<p>batch gradient descent的每一步都要遍历整个训练集,所以当我们的训练样本的量很大的时候,算法的开销很大;而stochastic gradient descent则每一步只选取了一个样本,因此后者的开销比前者要小很多,显然速度也要更快. 当然, 后者在有些情况下不会收敛,但是对于大多数的时间中,后者得到的结果都是真实最小值的一个足够好的近似结果</p>

<h2 id="正规方程">正规方程</h2>

<p>梯度下降是一种的到最小值的方法,但是我们有另一种方法,不需要通过迭代,一次性便可以得到最小值,那便是正规方程. 我们通过对J进行求导,然后令其为0.</p>

<p>$$J(\theta) = \sum<em>{i=0}^m(h</em>\theta(x^{(i)})-y^{(i)})^2 = \frac{1}{2}(\theta^TX-y)^T(\theta^TX-y)  $$</p>

<p>$$\nabla_{\theta}J(\theta)= 0$$</p>

<p>求解:
<img src="https://ws2.sinaimg.cn/bmiddle/006tNc79gy1fm1n3vpz95j31940s0n3b.jpg" alt="" /></p>

<h3 id="正规方程代码实现">正规方程代码实现</h3>

<p>参考了机器学习实战的代码实现</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">from</span> numpy <span style="color:#f92672">import</span> <span style="color:#f92672">*</span>

<span style="color:#75715e"># 读取数据集</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loadDataSet</span>(filename):
    file_stream <span style="color:#f92672">=</span> open(filename)
    num_feature <span style="color:#f92672">=</span> len(file_stream<span style="color:#f92672">.</span>readline()<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">&#39;</span>)) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>
    data_matrix <span style="color:#f92672">=</span> []
    label_matrix <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> line <span style="color:#f92672">in</span> file_stream<span style="color:#f92672">.</span>readlines():
        line_array <span style="color:#f92672">=</span> []
        <span style="color:#75715e"># strip remove leading and tail char</span>
        current_line <span style="color:#f92672">=</span> line<span style="color:#f92672">.</span>strip()<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">&#34;</span>)
        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num_feature):
            line_array<span style="color:#f92672">.</span>append(float(current_line[i]))
        data_matrix<span style="color:#f92672">.</span>append(line_array)
        label_matrix<span style="color:#f92672">.</span>append(float(current_line[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]))
    <span style="color:#66d9ef">return</span> data_matrix, label_matrix

<span style="color:#75715e"># 根据正规方程求theta</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">normalE_theta</span>(xVec, yVec):
    X <span style="color:#f92672">=</span> mat(xVec)
    Y <span style="color:#f92672">=</span> mat(yVec)<span style="color:#f92672">.</span>T
    XTX <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>T <span style="color:#f92672">*</span> X
    <span style="color:#66d9ef">if</span> linalg<span style="color:#f92672">.</span>det(XTX) <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:  <span style="color:#75715e"># 计算行列式,如果为奇异矩阵则不能求逆矩阵</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;This is a singular matrix, it canot be inverse&#39;</span>)
        <span style="color:#66d9ef">return</span>
    theta <span style="color:#f92672">=</span> XTX<span style="color:#f92672">.</span>I <span style="color:#f92672">*</span> X<span style="color:#f92672">.</span>T <span style="color:#f92672">*</span> Y
    <span style="color:#66d9ef">return</span> theta

<span style="color:#75715e"># 画图</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plotPoint</span>():
    xVec, yVec <span style="color:#f92672">=</span> loadDataSet(<span style="color:#e6db74">&#39;ex0.txt&#39;</span>)
    X <span style="color:#f92672">=</span> mat(xVec)
    Y <span style="color:#f92672">=</span> mat(yVec)
    fig <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>figure()
    ax <span style="color:#f92672">=</span> fig<span style="color:#f92672">.</span>add_subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
    ax<span style="color:#f92672">.</span>scatter(X[:, <span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>flatten()<span style="color:#f92672">.</span>A[<span style="color:#ae81ff">0</span>], Y<span style="color:#f92672">.</span>T[:, <span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>flatten()<span style="color:#f92672">.</span>A[<span style="color:#ae81ff">0</span>])
    xCopy <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>copy()
    xCopy<span style="color:#f92672">.</span>sort(<span style="color:#ae81ff">0</span>) <span style="color:#75715e"># sort(0) 中的指定排序方式</span>
    theta <span style="color:#f92672">=</span> normalE_theta(xVec, yVec)
    <span style="color:#66d9ef">print</span>(theta)
    yHat <span style="color:#f92672">=</span> xCopy <span style="color:#f92672">*</span> theta
    ax<span style="color:#f92672">.</span>plot(xCopy[:, <span style="color:#ae81ff">1</span>], yHat)
    plt<span style="color:#f92672">.</span>show()

plotPoint()</code></pre></div></div>
  
  <footer class="post-footer">
    <ul class="post-tags">
      
      
      <li><a href="https://blog.bing0ne.com/tags/ml">ML</a></li>
      
    </ul>
  </footer>
  
  
  
  
  <div id="disqus_thread"></div>
  <script>
    var disqus_shortname = 'bing0ne';
    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>
    Please enable JavaScript to view the
    <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  
  
  
</article>

</main>
<footer class="footer">
  <span>&copy; 2019 <a href="https://blog.bing0ne.com">bing0ne Base</a></span>
  <span>&middot;</span>
  <span>Powered by <a href="https://gohugo.io/" rel="noopener" target="_blank">Hugo️️</a>️</span>
  <span>&middot;</span>
  <span>Theme️ <a href="https://github.com/nanxiaobei/hugo-paper" rel="noopener" target="_blank">Paper</a></span>
</footer>
<script src="https://blog.bing0ne.com/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
</body>
</html>

