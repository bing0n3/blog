<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content=bing0ne><meta name=description content=决策树分类法是一种简单但却广泛使用的分类技术。它相较于kNN算法，能够展示给我们更多关于平均实例样本和典型实力样本的特征。决策树是一种使用概率测量方法处理分类问题的算法。><meta name=keywords content=blog,bing0ne,gopher,gsoc><link rel=canonical href=https://blog.bing0ne.com/posts/id3-algorithm/><title>决策树算法 --ID3 &middot; bing0ne Base</title><link rel="shortcut icon" href=https://blog.bing0ne.com/images/favicon.ico><link rel=stylesheet href=https://blog.bing0ne.com/css/animate.min.css><link rel=stylesheet href=https://blog.bing0ne.com/css/remixicon.css><link rel=stylesheet href=https://blog.bing0ne.com/css/zozo.css><link rel=stylesheet href=https://blog.bing0ne.com/css/highlight.css><script>(function(d){var config={kitId:'kfn1kkp',scriptTimeout:3000,async:true},h=d.documentElement,t=setTimeout(function(){h.className=h.className.replace(/\bwf-loading\b/g,"")+" wf-inactive";},config.scriptTimeout),tk=d.createElement("script"),f=false,s=d.getElementsByTagName("script")[0],a;h.className+=" wf-loading";tk.src='https://use.typekit.net/'+config.kitId+'.js';tk.async=true;tk.onload=tk.onreadystatechange=function(){a=this.readyState;if(f||a&&a!="complete"&&a!="loaded")return;f=true;clearTimeout(t);try{Typekit.load(config)}catch(e){}};s.parentNode.insertBefore(tk,s)})(document);</script></head><body><div class="main animated"><div class="nav_container animated fadeInDown"><div class=site_nav id=site_nav><ul><li><a href=/>Home</a></li><li><a href=/posts/>Archive</a></li><li><a href=/tags/>Tags</a></li></ul></div><div class=menu_icon><a id=menu_icon><i class=remixicon-links-line></i></a></div></div><div class="header animated fadeInDown"><div class=site_title_container><div class=site_title><h1><a href=https://blog.bing0ne.com><span>bing0ne Base</span>
<img src=https://blog.bing0ne.com/images/logo.svg></a></h1></div><div class=description><p class=sub_title>Neverland</p><div class=my_socials><a href=https://github.com/bing0n3 title=github target=_blank><i class=remixicon-github-fill></i></a><a href=https://instagram.com/bing0n3 title=instagram target=_blank><i class=remixicon-instagram-fill></i></a><a href=https://twitter.com/bing0n3 title=twitter target=_blank><i class=remixicon-twitter-fill></i></a><a href=https://blog.bing0ne.com/index.xml type=application/rss+xml title=rss target=_blank><i class=remixicon-rss-fill></i></a></div></div></div></div><div class=content><div class=post_page><div class="post animated fadeInDown"><div class="post_title post_detail_title"><h2><a href=/posts/id3-algorithm/>决策树算法 --ID3</a></h2><span class=date>2016.12.02</span></div><div class="post_content markdown"><p>决策树分类法是一种简单但却广泛使用的分类技术。它相较于kNN算法，能够展示给我们更多关于平均实例样本和典型实力样本的特征。决策树是一种使用概率测量方法处理分类问题的算法。</p><h1 id=如何建立决策树>如何建立决策树</h1><p>在原则上，对于一个给定的数据集，我们所可以构造的决策树的数目达到了指数级。因此，找到一个最佳决策树在计算上是不可行的。但尽管如此，人们还是找到了一些方法，去寻找具有一定的准确率的次最优决策树。我们采取贪心的策略，在每一次选择划分数据的属性时，选择局部最优的属性来构造决策。这就是Hunt算法。Hunt算法是一种通过将训练集划分成子集的一种以递归方法建立决策树的算法。</p><h1 id=id3算法的概述>ID3算法的概述</h1><p>对于一个决策树算法最关键的一点，就是如何选择特征作为划分数据集的标准。ID3算法就是一个选择信息增益作为其划分依据的算法。它选择信息增益最大的特征来对数据集进行划分。
还有一点ID3算法需要解决的就是如何判定划分的结束。这有两种情况，一种是划分出来的所有类属于同一个类。第二种情况，便是没有属性可以用来划分。</p><h1 id=划分数据的依据>划分数据的依据</h1><p>ID3算法是依靠信息增益作为衡量标准的决策树算法。</p><h2 id=信息熵-entropy>信息熵(Entropy)</h2><p>熵的概念主要是指信息的混乱程度，变量的不确定性越大，熵的值也就越大，熵的公式可以表示为:</p><p>$$Entropy(t): -\sum_{i=0}^{c-1} p(i|t) log_2p(i|t)$$</p><h2 id=信息增益-information-gain>信息增益（Information gain）</h2><p>信息增益指的是划分前后熵的变化，可以用下面的公式表示：</p><p>$$ InfoGain: I(parent) - \sum_{j =1}^{k} \frac{N(v_j)}{N} I(v_j)$$</p><p>其中k表示划分后所具有的特征数目，N表示条目数量。I是熵。</p><h1 id=id3算法实现>ID3算法实现</h1><h2 id=计算熵>计算熵</h2><p>首先统计出有多少个特征，然后计算出每个特征的概率，并按照计算熵的公式计算。</p><pre><code class=language-python>def calcEnt(dataSet):
    numEntries: len(dataSet)
    labelsCount: {} 
    #计算每个label对应的条目数量
    for featureVec in dataSet:
        currentLable: featureVec[-1]
        labelsCount[currentLable]: labelsCount.get(currentLable, 0) + 1
    ent: 0.0
    # 计算熵
    for key in labelsCount:
        prob: labelsCount.get(key) / float(numEntries)
        ent -= prob * log(prob, 2)
    return ent
</code></pre><h2 id=划分数据集>划分数据集</h2><p>根据输入的特征类别和该特征的值从数据集划分出一个子数据集。</p><pre><code class=language-python>def splitDataSet(dataSet, axis, value):
    retDataSet: []
    for data in dataSet:
        if data[axis] == value:
            # 将符合值且去除axis的特征加入到返回的List中
            reducedFeature: data[:axis]
            reducedFeature.extend(data[axis + 1:])
            retDataSet.append(reducedFeature)
    return retDataSet
</code></pre><h2 id=选择信息增益最大的划分特征>选择信息增益最大的划分特征</h2><p>从数据集中选择出当前最优的划分特征。根据信息增益的计算公式。</p><pre><code class=language-python>def chooseBestFeatureToSplit(dataSet):
'''
return the index of the best feature
'''
numFeatures: len(dataSet[0]) - 1
numDataSet: len(dataSet)
# 计算未划分时的熵
baseEntropy: calcEnt(dataSet)
bestInoGain: 0.0
bestFeature: 0.0
for i in range(numFeatures):
    # 得到第i个特征对应的值们
    valsList: [data[i] for data in dataSet]
    uniqueVals: set(valsList)
    newEntropy: 0.0
    # 计算每个值情况下的熵以及该情况发生的概率
    for val in uniqueVals:
        subDataSet: splitDataSet(dataSet, i, val)
        prob: len(subDataSet) / float(numDataSet)
        newEntropy += prob * calcEnt(subDataSet)
    infoGain: float(baseEntropy - newEntropy)
    if(infoGain &gt; bestInoGain):
        bestInoGain: infoGain
        bestFeature: i
return bestFeatur

</code></pre><h2 id=选择最多的类别>选择最多的类别</h2><p>如上文所述，我们有两种不同的结束判定情况。
这里我们介绍的是当数据集中没有可划分的特征，但是留下来了不同的类。我们为了输出方便，从中选择数目最多的类输出</p><pre><code class=language-python>def majorityCnt(classList):
    classCount: {}
    for vec in classList:
        classCount[vec]: classCount.get(vec, 0) + 1
    sortedClassCount: sorted(
        classCount.items(), key=operator.itemgetter(1), reverse=True)
    return sortedClassCount[0][0]
</code></pre><h2 id=构造树>构造树</h2><p>当我们完成前面那些构造树的函数后，我们便可以正式构造我们的函数。这是一个递归算法。</p><pre><code class=language-python>def createTree(dataSet, labels):
'''
label 用于辅助构造树，提供特征的名字
'''
    # 每个数据对应的类别
    classList: [subData[-1] for subData in dataSet]
    # 结束情况一 只有同一类的数据
    if classList.count(classList[0]) == len(classList):
        return classList[0]
    # 结束情况二 没有可以划分的特征
    if len(dataSet[0]) == 1:
        return majorityCnt(classList)
    # 选择最佳的划分特征
    bestFeature: chooseBestFeatureToSplit(dataSet)
    bestFeatureLabel: labels[bestFeature]
    myTree: {bestFeatureLabel: {}}
    # 删除已被选择的特征
    del(labels[bestFeature])
    # 得到用于划分的特征对应的值们
    featureVals: [example[bestFeature] for example in dataSet]
    uniqueVals: set(featureVals)
    # 对每一个值 递归调用createTree，来继续划分数据集
    for values in uniqueVals:
        # 赋值给subLabels，而不是引用
        subLabels: labels[:]
        myTree[bestFeatureLabel][values]: createTree(
            splitDataSet(dataSet, bestFeature, values), subLabels)
    return myTree
</code></pre><h2 id=分类>分类</h2><p>当构造完树后，我们就能用树来进行分类。这里的<code>inputree</code>参数便是我们前面构造的树。</p><pre><code class=language-python>def classify(inputTree, featureLabels, testVec):
    firstLabel: list(inputTree.keys())[0]
    secondTree: inputTree.get(firstLabel)   # seconTree 就是firstLabel对应的value
    featureIndex: featureLabels.index(firstLabel)
    classLabel: ''
    for key in secondTree:

        if testVec[featureIndex] == key:
            if type(secondTree[key]).__name__ == 'dict':
                classLabel: classify(secondTree[key], featureLabels, testVec)
            else:
                classLabel: secondTree[key]
    return classLabel
</code></pre></div><div class=post_footer><div class=meta><div class=info><span class="field tags"><i class=remixicon-stack-line></i><a href=https://blog.bing0ne.com/tags/ml/>ML</a></span></div></div></div></div><div class=disqus><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"bing0ne"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div></div><a id=back_to_top href=# class=back_to_top><span>△</span></a></div><footer class=footer><div class=powered_by><a href=https://zeuk.me>Designed by Zeuk,</a>
<a href=http://www.gohugo.io/>Proudly published with Hugo</a></div><div class=footer_slogan><span>Share Memory by Communication</span></div></footer><script src=https://blog.bing0ne.com/js/jquery-3.3.1.min.js></script><script src=https://blog.bing0ne.com/js/zozo.js></script><script src=https://blog.bing0ne.com/js/highlight.pack.js></script><link href=https://blog.bing0ne.com/css/fancybox.min.css rel=stylesheet><script src=https://blog.bing0ne.com/js/fancybox.min.js></script><script>hljs.initHighlightingOnLoad()</script><script type=text/javascript async src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\[\[','\]\]']],processEscapes:true,processEnvironments:true,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}});MathJax.Hub.Queue(function(){var all=MathJax.Hub.getAllJax(),i;for(i=0;i<all.length;i+=1){all[i].SourceElement().parentNode.className+=' has-jax';}});</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-71832126-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></body></html>